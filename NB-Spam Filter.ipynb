{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification using a Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Email messages can be classified as _Spam_ or _Ham_ using a simple naive bayes classifier using a _Bag of Words_ model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "\n",
    "The training data can be downloaded from the [TREC 07 spam corpus](http://plg.uwaterloo.ca/~gvcormac/treccorpus07/).\n",
    "It consists of around 75K email messages tagged as spam or ham and their distribution is about 2/3 Spam and 1/3 Ham.\n",
    "\n",
    "### Parsing the messages\n",
    "\n",
    "For this notebook, we will consider only subjects and text content. The data needs to be preprocessed for the sake of dimentionality reduction by doing:\n",
    "\n",
    "* Casefolding\n",
    "* URL normalization\n",
    "* Stop word filtering\n",
    "* Porter stemming\n",
    "\n",
    "A snippet from `message.py` showing the code implementation can be read:\n",
    "\n",
    "```python\n",
    "def processedText(self):\n",
    "    ''' Stems and filters stop words, among other things '''\n",
    "\n",
    "    try:\n",
    "        # Concatenate subject and body\n",
    "        txt = self.subject.lower() + '\\n' + self.text.lower()\n",
    "        # Replace URLs for a token\n",
    "        txt = url.sub('-URL-', txt)\n",
    "        # Tokenize\n",
    "        tokens = nltk.word_tokenize(txt)\n",
    "        # Stem and remove stop words\n",
    "        stems = [self.stemmer.stem(w) for w in tokens if w not in stopwords]\n",
    "    except:\n",
    "        stems = []\n",
    "\n",
    "    return ' '.join(stems)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from message import Message\n",
    "\n",
    "DATA_DIR='data/data'\n",
    "LABELS_PATH='data/partial/index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read messages\n",
    "print \"Reading messages ...\"\n",
    "start = datetime.now()\n",
    "\n",
    "messages = []\n",
    "heldout_messages = {}\n",
    "for path in glob.iglob(os.path.join(DATA_DIR, '*')):\n",
    "    m = Message.load(path)\n",
    "    messages.append(m)\n",
    "\n",
    "# Read the labels\n",
    "print \"Reading labels ...\"\n",
    "\n",
    "with open(LABELS_PATH) as f:\n",
    "    labels_dict = {}\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        label, key = line.split()\n",
    "        labels_dict[key.split('/')[-1]] = 1 if label.lower() == 'ham' else 0\n",
    "        \n",
    "end = datetime.now()\n",
    "\n",
    "print \"%i seconds reading and parsing ...\" % (end - start).seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bag of words model\n",
    "A text is represented as a multiset containing its words disregarding their order. Imagine [dumping all the words in a document into a bag](https://en.wikipedia.org/wiki/Bag-of-words_model). Despite losing syntax and structure, this representation gives good results for this task.\n",
    "\n",
    "### Vector representation\n",
    "Each bag of words is represented by an integer vector whose dimentionality is the _size of the vocabulary_ and each entry contains the _term frequency_ for a given word in the message's vector. This yields very sparse vectors, because each message contains a very small subset of the words in the vocabulary, thus, _the majority of the entries will be zeroes_.\n",
    "\n",
    "$$\\mathbf{d}_i = [0, 0, 2, ..., 0, 5, 0]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data in 70% training, 30% testing\n",
    "print \"Creating feature vectors ...\"\n",
    "start = datetime.now()\n",
    "\n",
    "vectorizer = sk.feature_extraction.text.CountVectorizer()\n",
    "X = vectorizer.fit_transform([m.processedText for m in messages])\n",
    "y = np.array([labels_dict[m.id] for m in messages])\n",
    "\n",
    "end = datetime.now()\n",
    "\n",
    "print \"%i seconds extracting features ...\" % (end - start).seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Naive Bayes classifier\n",
    "\n",
    "The intiution behind _Naive Bayes_ is to find the probability of each class, in this case __Spam__ and __Ham__ given a message and select whichever class is more likely:\n",
    "\n",
    "$$ \\hat{C} = \\arg\\max_C P(C \\mid \\mathbf{d}_i)$$\n",
    "\n",
    "We use _Baye's rule_ to turn the probability around:\n",
    "\n",
    "$$P(C \\mid \\mathbf{d}_i) = P(\\mathbf{d}_i \\mid C)\\frac{P(C)}{P(\\mathbf{d}_i)}$$\n",
    "\n",
    "Since we are not concerned with the extact probability of each class, we can dismiss the denominator and the maximum of the resulting likelihood will happen at the same point as with the probabilities:\n",
    "\n",
    "$$P(C \\mid \\mathbf{d}_i) \\propto P(\\mathbf{d}_i \\mid C)P(C)$$\n",
    "\n",
    "## Naive assumption\n",
    "\n",
    "To simplify the model, we assume _conditional independence_ of the words in a document, given the class. This is the naive assumption of the model, which reduces the number of estimated parameters to a linear function of the size vocabulary. The new likelihood formula is:\n",
    "\n",
    "$$P(C \\mid \\mathbf{d}_i) \\propto \\prod_{j = 1}^{|V|}P(d_{ij}\\mid C)P(C)$$\n",
    "\n",
    "## Multinomial distribution and out-of-vocabulary words\n",
    "\n",
    "Our feature vectors contain term frequencies, which are integer numbers. A term's conditional pobability distribution can be naturaly modeled by a [multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution). The parameters are estimated using _Maximum Likelihood Estimation_, which in this case is counting the ocurrences of each team across the messages of a given class _C_. The prior's (Spam or Ham) marginal distribution is computed the same way.\n",
    "\n",
    "It is possible that a message contains a word not seen during training. This would break the product, as it's probability would be zero and drag down the whole computation. To handle this, a new term representing the _OOV_ words is introduced and a small piece of probability mass is transfered to it. This is known as [Additive Smoothing](add one smoothing trigram) and guarantees that no term in the product will ever be zero, making the computation feasible.\n",
    "\n",
    "## Dealing with numerical _underflow_\n",
    "\n",
    "By definition, probabilities range from 0 to 1, and the joint probability of a message given a class will be a _very tiny number_ resulting from multiplying tens of thousands or probably hundreds of thousands of individual term conditional probabilities. Theoretically there's no problem with this, but computers can't handle such small quantities with their floating point representation because they become _zero_ very quickly.\n",
    "\n",
    "To handle this, we instead maximize _log probabilities_, and our target maximization becomes:\n",
    "\n",
    "$$ \\hat{C} = \\arg\\max_C \\log P(C \\mid \\mathbf{d}_i)$$\n",
    "\n",
    "$$\\log P(C \\mid \\mathbf{d}_i) \\propto \\log P(\\mathbf{d}_i \\mid C) + \\log P(C)$$\n",
    "\n",
    "$$\\log P(C \\mid \\mathbf{d}_i) \\propto \\sum_{j = 1}^{|V|} \\log P(d_{ij}\\mid C) + \\log P(C)$$\n",
    "\n",
    "Maximizing this expression gives the same result as maximizing the original because the logarithm is a [monotonic function](https://en.wikipedia.org/wiki/Monotonic_function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing NB with code\n",
    "\n",
    "Fortunately, all these computations have been already implemented by [Scikit Learn](http://scikit-learn.org/stable/modules/naive_bayes.html) and all we need to do is write a couple lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data set in training 70% and testing 30%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.7)\n",
    "\n",
    "print \"Training ...\"\n",
    "start = datetime.now()\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "end = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Testing:\"\n",
    "y_pred = nb.predict(X_test)\n",
    "print classification_report(y_test, y_pred, target_names=['Spam', 'Ham'])\n",
    "print\n",
    "print \"Accuracy: %.3f\" % accuracy_score(y_test, y_pred)\n",
    "print\n",
    "print \"Confusion matrix:\"\n",
    "print confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links\n",
    "* [More on text classification](http://nlp.stanford.edu/IR-book/html/htmledition/text-classification-and-naive-bayes-1.html)\n",
    "* [Wikipedia entry for Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "* [Scikit Learn](http://scikit-learn.org/)\n",
    "* [NLTK](http://www.nltk.org)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
